{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBGD(0/49): loss=53.273803832842624, w=0.3449989433030187, b=13.319072130000002\n",
      "MiniBGD(0/49): loss=42.18229217130122, w=0.0775130246081342, b=23.8676374892463\n",
      "MiniBGD(0/49): loss=33.349670317994025, w=-1.4999953967585387, b=32.290164404699425\n",
      "MiniBGD(0/49): loss=25.990099650381588, w=-2.239453412103739, b=38.82828700064334\n",
      "MiniBGD(0/49): loss=22.572614489029053, w=-1.8848142529044412, b=44.46800833797148\n",
      "MiniBGD(0/49): loss=16.621845280712655, w=-2.0515574115306197, b=48.62686497958561\n",
      "MiniBGD(0/49): loss=14.042443065553794, w=-1.5755686874199433, b=52.13704757866034\n",
      "MiniBGD(0/49): loss=12.783042153765368, w=-0.041866827447825106, b=55.444667178455454\n",
      "MiniBGD(0/49): loss=9.666762813915929, w=0.7972147490603838, b=57.906223892911996\n",
      "MiniBGD(0/49): loss=6.01856140229046, w=0.5950957587800346, b=59.42309892000397\n",
      "MiniBGD(0/49): loss=4.686569915617138, w=0.4047807902136472, b=60.60587528027796\n",
      "MiniBGD(0/49): loss=3.8333515143404275, w=0.6871350290801196, b=61.45521643355505\n",
      "MiniBGD(0/49): loss=4.473390070638829, w=0.9524922995426617, b=62.432480658744325\n",
      "MiniBGD(0/49): loss=4.787114264131789, w=1.5268977662707162, b=63.6603614370538\n",
      "MiniBGD(0/49): loss=2.009969780926415, w=1.485611111088085, b=63.992299521925894\n",
      "MiniBGD(0/49): loss=2.882860760757297, w=1.7711559623466884, b=64.4880749683092\n",
      "MiniBGD(0/49): loss=3.389491270803344, w=2.3263756047298387, b=65.14767179587724\n",
      "MiniBGD(0/49): loss=2.470225762756511, w=2.7519949580171574, b=65.50277531975833\n",
      "MiniBGD(0/49): loss=3.7703188113054273, w=1.213853165910629, b=65.30657914835267\n",
      "MiniBGD(0/49): loss=2.315894285780742, w=1.468162789648494, b=65.5415883930581\n",
      "MiniBGD(0/49): loss=2.2840271845999722, w=1.9484871731341795, b=65.79729532371661\n",
      "MiniBGD(0/49): loss=2.5066441527505794, w=2.2503071757843482, b=65.95867395205441\n",
      "MiniBGD(0/49): loss=1.2520370333770594, w=2.3797032247063186, b=65.76930597298403\n",
      "MiniBGD(0/49): loss=1.7408379647152543, w=2.5742485577247933, b=66.03832423888541\n",
      "MiniBGD(0/49): loss=2.12794415959766, w=2.780711909119201, b=65.93288847784817\n",
      "MiniBGD(0/49): loss=1.8071610904057238, w=2.9311670464401383, b=65.97916930371403\n",
      "MiniBGD(0/49): loss=2.0226781059132293, w=3.1136397534168188, b=66.21308768035507\n",
      "MiniBGD(0/49): loss=1.6342929134325055, w=3.285892415084477, b=66.44983473211279\n",
      "MiniBGD(0/49): loss=2.186486819935767, w=3.5565381634463287, b=66.62012483343581\n",
      "MiniBGD(0/49): loss=1.664368188612888, w=3.70579865603231, b=66.73228027593159\n",
      "MiniBGD(0/49): loss=1.6148210804410474, w=3.8501826863407205, b=66.70316366664993\n",
      "MiniBGD(0/49): loss=1.5949642849352366, w=4.051209302608795, b=66.51238335946437\n",
      "MiniBGD(0/49): loss=1.3027739097163527, w=4.198220478639336, b=66.57253854131574\n",
      "MiniBGD(0/49): loss=0.8313462173268, w=4.261151818914051, b=66.63883531514473\n",
      "MiniBGD(0/49): loss=1.4192333245166828, w=4.427009725883835, b=66.75294988893535\n",
      "MiniBGD(0/49): loss=0.49388358157501955, w=4.472265162076929, b=66.8804369839773\n",
      "MiniBGD(1/49): loss=0.9219160955139643, w=4.508044958976963, b=66.90347797487269\n",
      "MiniBGD(2/49): loss=1.5888648407759667, w=4.660828975705165, b=66.5892315396541\n",
      "MiniBGD(3/49): loss=0.7036814516917496, w=4.692794464059909, b=66.66091721873329\n",
      "MiniBGD(4/49): loss=1.541808559277723, w=4.776761431449868, b=66.74142327638488\n",
      "MiniBGD(5/49): loss=0.7477538269094446, w=4.825354204084237, b=66.69307985188092\n",
      "MiniBGD(6/49): loss=0.9308090981030404, w=4.967806926005828, b=66.57248005781364\n",
      "MiniBGD(7/49): loss=1.3385045377488587, w=4.961775202405584, b=66.68305496645517\n",
      "MiniBGD(8/49): loss=1.3924461769206764, w=5.0105931791584535, b=66.76293441343421\n",
      "MiniBGD(9/49): loss=1.292160237938924, w=5.181830772137017, b=66.6287865841772\n",
      "MiniBGD(10/49): loss=1.1863162046665294, w=5.233933829138019, b=66.57345441256435\n",
      "MiniBGD(11/49): loss=0.6521190003686079, w=5.199816833304369, b=66.54287703361054\n",
      "MiniBGD(12/49): loss=1.3664210118607634, w=5.241388641319328, b=66.67844298664671\n",
      "MiniBGD(13/49): loss=1.5085804193989347, w=5.339548946408287, b=66.6859007271404\n",
      "MiniBGD(14/49): loss=1.4007077076859105, w=5.414107542250347, b=66.46464133588569\n",
      "MiniBGD(15/49): loss=1.0309019226861529, w=5.456079045517494, b=66.49597090252472\n",
      "MiniBGD(16/49): loss=1.1815313045622182, w=5.456343484572847, b=66.52760708445439\n",
      "MiniBGD(17/49): loss=1.4089824919288716, w=5.489664298519985, b=66.5163556922472\n",
      "MiniBGD(18/49): loss=1.091587810699209, w=5.567831684114588, b=66.51659991169376\n",
      "MiniBGD(19/49): loss=1.1321394465037578, w=5.6354470823212655, b=66.64312910887757\n",
      "MiniBGD(20/49): loss=1.0205454610747082, w=5.62645385516492, b=66.4962631350193\n",
      "MiniBGD(21/49): loss=1.0289303163362404, w=5.750724640800133, b=66.6778655930542\n",
      "MiniBGD(22/49): loss=1.2126571312249332, w=5.816210983021844, b=66.68910423731029\n",
      "MiniBGD(23/49): loss=1.330337009382913, w=5.9204977350957, b=66.7176904525457\n",
      "MiniBGD(24/49): loss=1.0705808068168523, w=5.921367353405616, b=66.77154290780241\n",
      "MiniBGD(25/49): loss=1.0343659328012293, w=5.8488878315805, b=66.98458963109482\n",
      "MiniBGD(26/49): loss=1.295823478747525, w=5.890452908357421, b=66.86697290449754\n",
      "MiniBGD(27/49): loss=0.8802838381041184, w=5.933370656708735, b=66.89501811157807\n",
      "MiniBGD(28/49): loss=1.7776453957569125, w=5.912773312462833, b=66.97125206372925\n",
      "MiniBGD(29/49): loss=1.1699532784969342, w=5.896029362511469, b=66.93044148923582\n",
      "MiniBGD(30/49): loss=0.7187883359904206, w=5.846834332792689, b=67.02787617090178\n",
      "MiniBGD(31/49): loss=1.1798338888266855, w=5.819404721920838, b=66.92218081116411\n",
      "MiniBGD(32/49): loss=1.0866642551537091, w=5.874296657188693, b=66.83243130752699\n",
      "MiniBGD(33/49): loss=1.5653404324465285, w=6.021019315056023, b=66.77675731353771\n",
      "MiniBGD(34/49): loss=1.050986993010153, w=6.047381348792052, b=66.69406420585362\n",
      "MiniBGD(35/49): loss=0.9340041554478503, w=6.007152686065281, b=66.65084807724438\n",
      "MiniBGD(36/49): loss=1.3019940024507215, w=5.990072346473138, b=66.64296407967326\n",
      "MiniBGD(37/49): loss=0.9151175017344599, w=5.987584677657691, b=66.66844193622474\n",
      "MiniBGD(38/49): loss=1.1082982882737262, w=6.037642694560332, b=66.65942774389839\n",
      "MiniBGD(39/49): loss=1.04134690008742, w=6.063545727376041, b=66.69138796562659\n",
      "MiniBGD(40/49): loss=1.429451544251623, w=6.110659639376117, b=66.604094537556\n",
      "MiniBGD(41/49): loss=0.9712225531744728, w=6.14981006687342, b=66.65966457059159\n",
      "MiniBGD(42/49): loss=1.2755543510953609, w=6.192572221846946, b=66.68483413324083\n",
      "MiniBGD(43/49): loss=0.8223396391880818, w=6.229483068811923, b=66.66011869913696\n",
      "MiniBGD(44/49): loss=0.7535544427471024, w=6.227684325609609, b=66.71390248059302\n",
      "MiniBGD(45/49): loss=1.0122793513561696, w=6.280067548297381, b=66.69065728102667\n",
      "MiniBGD(46/49): loss=1.1661344790121242, w=6.288315492620099, b=66.61244583138443\n",
      "MiniBGD(47/49): loss=0.784092710340419, w=6.192641597739517, b=66.60240615441158\n",
      "MiniBGD(48/49): loss=1.4820006654924973, w=6.228658167673032, b=66.70927204556995\n",
      "MiniBGD(49/49): loss=1.26986116457821, w=6.210459048038594, b=66.84868786755676\n",
      "Learn: execution time=0.051 seconds\n",
      "R2: 0.616710465217146\n",
      "MSE: 17.047081658114237\n",
      "RMSE: 4.1288111676503485\n",
      "MAE: 1.5503804927849583\n",
      "List: [array([ 0.34499894, 13.31907213]), array([ 0.07751302, 23.86763749]), array([-1.4999954, 32.2901644]), array([-2.23945341, 38.828287  ]), array([-1.88481425, 44.46800834]), array([-2.05155741, 48.62686498]), array([-1.57556869, 52.13704758]), array([-4.18668274e-02,  5.54446672e+01]), array([ 0.79721475, 57.90622389]), array([ 0.59509576, 59.42309892]), array([ 0.40478079, 60.60587528]), array([ 0.68713503, 61.45521643]), array([ 0.9524923 , 62.43248066]), array([ 1.52689777, 63.66036144]), array([ 1.48561111, 63.99229952]), array([ 1.77115596, 64.48807497]), array([ 2.3263756, 65.1476718]), array([ 2.75199496, 65.50277532]), array([ 1.21385317, 65.30657915]), array([ 1.46816279, 65.54158839]), array([ 1.94848717, 65.79729532]), array([ 2.25030718, 65.95867395]), array([ 2.37970322, 65.76930597]), array([ 2.57424856, 66.03832424]), array([ 2.78071191, 65.93288848]), array([ 2.93116705, 65.9791693 ]), array([ 3.11363975, 66.21308768]), array([ 3.28589242, 66.44983473]), array([ 3.55653816, 66.62012483]), array([ 3.70579866, 66.73228028]), array([ 3.85018269, 66.70316367]), array([ 4.0512093 , 66.51238336]), array([ 4.19822048, 66.57253854]), array([ 4.26115182, 66.63883532]), array([ 4.42700973, 66.75294989]), array([ 4.47226516, 66.88043698]), array([ 4.50804496, 66.90347797]), array([ 4.66082898, 66.58923154]), array([ 4.69279446, 66.66091722]), array([ 4.77676143, 66.74142328]), array([ 4.8253542 , 66.69307985]), array([ 4.96780693, 66.57248006]), array([ 4.9617752 , 66.68305497]), array([ 5.01059318, 66.76293441]), array([ 5.18183077, 66.62878658]), array([ 5.23393383, 66.57345441]), array([ 5.19981683, 66.54287703]), array([ 5.24138864, 66.67844299]), array([ 5.33954895, 66.68590073]), array([ 5.41410754, 66.46464134]), array([ 5.45607905, 66.4959709 ]), array([ 5.45634348, 66.52760708]), array([ 5.4896643 , 66.51635569]), array([ 5.56783168, 66.51659991]), array([ 5.63544708, 66.64312911]), array([ 5.62645386, 66.49626314]), array([ 5.75072464, 66.67786559]), array([ 5.81621098, 66.68910424]), array([ 5.92049774, 66.71769045]), array([ 5.92136735, 66.77154291]), array([ 5.84888783, 66.98458963]), array([ 5.89045291, 66.8669729 ]), array([ 5.93337066, 66.89501811]), array([ 5.91277331, 66.97125206]), array([ 5.89602936, 66.93044149]), array([ 5.84683433, 67.02787617]), array([ 5.81940472, 66.92218081]), array([ 5.87429666, 66.83243131]), array([ 6.02101932, 66.77675731]), array([ 6.04738135, 66.69406421]), array([ 6.00715269, 66.65084808]), array([ 5.99007235, 66.64296408]), array([ 5.98758468, 66.66844194]), array([ 6.03764269, 66.65942774]), array([ 6.06354573, 66.69138797]), array([ 6.11065964, 66.60409454]), array([ 6.14981007, 66.65966457]), array([ 6.19257222, 66.68483413]), array([ 6.22948307, 66.6601187 ]), array([ 6.22768433, 66.71390248]), array([ 6.28006755, 66.69065728]), array([ 6.28831549, 66.61244583]), array([ 6.1926416 , 66.60240615]), array([ 6.22865817, 66.70927205]), array([ 6.21045905, 66.84868787])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "This is an example to perform simple linear regression algorithm on the dataset (weight and height),\n",
    "where x = weight and y = height.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from utilities.losses import compute_loss\n",
    "from utilities.optimizers import gradient_descent, pso, mini_batch_gradient_descent\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# General settings\n",
    "from utilities.visualization import visualize_train, visualize_test\n",
    "# Build baseline model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 309\n",
    "# Freeze the random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "train_test_split_test_size = 0.3\n",
    "\n",
    "# Training settings\n",
    "alpha = 0.1  # step size\n",
    "max_iters = 50  # max iterations\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Data from CSV\n",
    "    :return: df    a panda data frame\n",
    "    \"\"\"\n",
    "    # df = pd.read_csv(\"../data/Part2.csv\")\n",
    "    df=pd.read_csv(\"../data/Part2Outliers.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def data_preprocess(data):\n",
    "    \"\"\"\n",
    "    Data preprocess:\n",
    "        1. Split the entire dataset into train and test\n",
    "        2. Split outputs and inputs\n",
    "        3. Standardize train and test\n",
    "        4. Add intercept dummy for computation convenience\n",
    "    :param data: the given dataset (format: panda DataFrame)\n",
    "    :return: train_data       train data contains only inputs\n",
    "             train_labels     train data contains only labels\n",
    "             test_data        test data contains only inputs\n",
    "             test_labels      test data contains only labels\n",
    "             train_data_full       train data (full) contains both inputs and labels\n",
    "             test_data_full       test data (full) contains both inputs and labels\n",
    "    \"\"\"\n",
    "    # Split the data into train and test\n",
    "    train_data, test_data = train_test_split(data, test_size = train_test_split_test_size)\n",
    "\n",
    "    # Pre-process data (both train and test)\n",
    "    train_data_full = train_data.copy()\n",
    "    train_data = train_data.drop([\"Height\"], axis = 1)\n",
    "    train_labels = train_data_full[\"Height\"]\n",
    "\n",
    "    test_data_full = test_data.copy()\n",
    "    test_data = test_data.drop([\"Height\"], axis = 1)\n",
    "    test_labels = test_data_full[\"Height\"]\n",
    "\n",
    "    # Standardize the inputs\n",
    "    train_mean = train_data.mean()\n",
    "    train_std = train_data.std()\n",
    "    train_data = (train_data - train_mean) / train_std\n",
    "    test_data = (test_data - train_mean) / train_std\n",
    "\n",
    "    # Tricks: add dummy intercept to both train and test\n",
    "    train_data['intercept_dummy'] = pd.Series(1.0, index = train_data.index)\n",
    "    test_data['intercept_dummy'] = pd.Series(1.0, index = test_data.index)\n",
    "    return train_data, train_labels, test_data, test_labels, train_data_full, test_data_full\n",
    "\n",
    "\n",
    "def learn(y, x, theta, max_iters, alpha, optimizer_type, metric_type ):\n",
    "    \"\"\"\n",
    "    Learn to estimate the regression parameters (i.e., w and b)\n",
    "    :param y:                   train labels\n",
    "    :param x:                   train data\n",
    "    :param theta:               model parameter\n",
    "    :param max_iters:           max training iterations\n",
    "    :param alpha:               step size\n",
    "    :param optimizer_type:      optimizer type (default: Batch Gradient Descient): GD, SGD, MiniBGD or PSO\n",
    "    :param metric_type:         metric type (MSE, RMSE, R2, MAE). NOTE: MAE can't be optimized by GD methods.\n",
    "    :return: thetas              all updated model  parameters tracked during the learning course\n",
    "             losses             all losses tracked during the learning course\n",
    "    \"\"\"\n",
    "    thetas = None\n",
    "    losses = None\n",
    "    if optimizer_type == \"BGD\":\n",
    "        thetas, losses = gradient_descent(y, x, theta, max_iters, alpha, metric_type)\n",
    "    elif optimizer_type == \"MiniBGD\":\n",
    "        thetas, losses = mini_batch_gradient_descent(y, x, theta, max_iters, alpha, metric_type, mini_batch_size = 10)\n",
    "    elif optimizer_type == \"PSO\":\n",
    "        thetas, losses = pso(y, x, theta, max_iters, 100, metric_type)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"[ERROR] The optimizer '{ot}' is not defined, please double check and re-run your program.\".format(\n",
    "                ot = optimizer_type))\n",
    "    return thetas, losses\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Settings\n",
    "    metric_type = \"MAE\"  # MSE, RMSE, MAE, R2\n",
    "    optimizer_type = \"MiniBGD\"  # PSO, BGD, MiniBGD\n",
    "\n",
    "    # Step 1: Load Data\n",
    "    data = load_data()\n",
    "\n",
    "    # Step 2: Preprocess the data\n",
    "    train_data, train_labels, test_data, test_labels, train_data_full, test_data_full = data_preprocess(data)\n",
    "    \n",
    "    # Step 3: Learning Start\n",
    "    theta = np.array([0.0, 0.0])  # Initialize model parameter\n",
    "\n",
    "    start_time = datetime.datetime.now()  # Track learning starting time\n",
    "    thetas, losses = learn(train_labels.values, train_data.values, theta, max_iters, alpha, optimizer_type, metric_type)\n",
    "\n",
    "    end_time = datetime.datetime.now()  # Track learning ending time\n",
    "    exection_time = (end_time - start_time).total_seconds()  # Track execution time\n",
    "\n",
    "    # Step 4: Results presentation\n",
    "    print(\"Learn: execution time={t:.3f} seconds\".format(t = exection_time))\n",
    "\n",
    "    print(\"R2:\", -compute_loss(test_labels.values, test_data.values, thetas[-1], \"R2\"))  # R2 should be maximize\n",
    "    print(\"MSE:\", compute_loss(test_labels.values, test_data.values, thetas[-1], \"MSE\"))\n",
    "    print(\"RMSE:\", compute_loss(test_labels.values, test_data.values, thetas[-1], \"RMSE\"))\n",
    "    print(\"MAE:\", compute_loss(test_labels.values, test_data.values, thetas[-1], \"MAE\"))\n",
    "    print(\"List:\",thetas)\n",
    "    niter = max_iters\n",
    "    \n",
    "    # visualize_train(train_data_full, train_labels, train_data, thetas, losses, niter)\n",
    "    visualize_test(test_data_full, test_data, thetas)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
